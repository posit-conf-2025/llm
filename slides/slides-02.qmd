---
title: Anatomy of a conversation
subtitle: Programming with LLM APIs<br>A Beginner's Guide in R and Python
author: <code>posit::conf(2025)</code>
date: 2025-09-16

editor:
  render-on-save: true
---

# How to think about LLMs {.dark-blue background-image="assets/retro-mac.jpg" background-size="cover" background-position="bottom left"}

```{r}
source(here::here("slides/_incremental_slides.R"))
```

::: notes
Some motivational words by Joe urging everyone to approach LLMs with curiosity and experimentation rather than preconceived notions about limitations, while building understanding from the ground up through practical experience.
:::

## Think Empirically, Not Theoretically {.center .text-center}

## Think Empirically, Not Theoretically {.center .text-center}

- It's okay to treat LLMs as **black boxes**. \
  We're not going to focus on how they work internally

- **Just try it!** When wondering if an LLM can do something,\
  experiment rather than theorize

- You might think they could not possibly do things\
  _that they clearly can do today_

::: notes
- Understanding the technical details can lead to **bad intuition** about capabilities
- You might think "they could not possibly do things that they clearly can do today"
- Empirical testing reveals actual capabilities vs. theoretical limitations
:::

## Embrace the Experimental Process {.center .text-center}

## Embrace the Experimental Process {.center .text-center}

- **Don't worry about ROI** during exploration. \
  Focus on learning and engaging with the technology

- **Failure is valuable!** \
  _those are some of the most interesting conversations that we have_

- **It doesn't have to be a success.** \
  Attempts that don't work still provide insights

::: notes
- Use the **best models available** for experimentation
- Don't constrain yourself to "practical" applications initially
- Think of it as forming your own independent conclusions about usefulness
:::

## Start Simple, Build Understanding {.center .text-center}

## Start Simple, Build Understanding {.center .text-center}

- We're going to focus on the **core building blocks**.

- All the incredible things you see AI do \
  **decompose to just a few key ingredients**.

- Our goal is to **have fun and build intuition** \
  through hands-on experience.

::: notes
- Think **very generally** about what tools can do - they're as powerful as any software you can write
- Remember: **"Everything decomposes"** to the basic components once you understand the underlying APIs
- Build intuition through hands-on experience rather than theoretical study
- We want to have fun in this course! It's worth saying up front that our examples are not necessarily practical, but they're full of practical lessons.
:::


# [Anatomy of a Conversation]{.white} {.no-invert-dark-mode background-image="assets/eduard-delputte-1P6LZ8S8XJc-unsplash.jpg" background-size="cover" background-position="bottom left"}

## {.center}

![](assets/intro-conversation-01.png)
![](assets/intro-conversation-02.png){.fragment}

::: notes
Those conversations often look something like this:

You ask a question and ChatGPT responds.
You can keep talking to ChatGPT and it keeps responding,
almost like you're having a conversation with a person.

The entire conversation happens via HTTP requests and responses
:::

## What's an HTTP request?

::: notes
HTTP requests power the internet.
When you visit a website, your browser sends an HTTP request to the server hosting that website.
The server responds to the request by sending back the HTML of the webpage.
And then your browser makes a bunch more requests to get the images, stylesheets, and scripts that make up the page, etc.
:::

```{r}
#| output: asis

incremental_slides(
  pattern = "intro-conversation-google-.+[.]svg$",
  template = r"(::: {{.mt6}}
{{{{< include {path} >}}}}
:::)",
  collapse = "\n\n## What's an HTTP request?\n\n"
)
```

## Talking with ChatGPT happens via HTTP

::: notes
Talking to an LLM like ChatGPT also happens via HTTP requests.
When you send a message to ChatGPT, you're sending an HTTP request to OpenAI's servers, this time a `POST` request.

OpenAI processes the request, runs the model, gets the answer and sends it back to you as the response to that POST request.
:::

```{r}
#| output: asis

incremental_slides(
  pattern = "intro-conversation-chat-.+[.]svg$",
  template = r"(::: {{.mt6}}
{{{{< include {path} >}}}}
:::
  )",
  collapse = "\n\n## Talking with ChatGPT happens via HTTP\n\n"
)
```

## Messages have roles

```{r}
#| output: asis

incremental_slides(
  pattern = "intro-conversation-roles-.+[.]svg$",
  template = r"(::: {{.tc}}
{{{{< include {path} >}}}}
:::)",
  collapse = "\n\n## Messages have roles\n\n"
)
```

## Message roles

| Role        | Description                                                       |
|:-----------|:---------------------------------------------------------------|
| `system_prompt` | Instructions from the developer (that's you!)<br>to set the behavior of the assistant |
| `user`       | Messages from the person interacting<br>with the assistant        |
| `assistant`  | The AI model's responses to the user                           |

## Hello, ellmer and chatlas!

::: {.easy-columns}
::: tc
![](../assets/logos/ellmer.png){style="max-width: 100%; max-height: 500px"}

R
:::

::: tc
![](../assets/logos/chatlas.png){style="max-width: 100%; max-height: 500px"}

Python
:::
:::

## {.no-invert-dark-mode background-image="assets/ellmer-and-chatlas.webp" background-size="cover" background-position="center"}

## Hello, ellmer and chatlas! {transition="fade"}

::: {.smaller style="--code-font-size: 0.66em"}
::: col
[![](../assets/icons/r-color.svg){height="36px" alt="R"} ellmer]{.flex .items-center .gap-1}

```r
library(ellmer)
```
:::

::: col
[![](../assets/icons/python-icon-color.svg){height="36px" alt="Python"} chatlas]{.flex .items-center .gap-1}

```python
import chatlas
```
:::
:::

## Hello, ellmer and chatlas! {transition="fade"}

::: {.smaller style="--code-font-size: 0.66em"}
::: col
[![](../assets/icons/r-color.svg){height="36px" alt="R"} ellmer]{.flex .items-center .gap-1}

```{.r code-line-numbers="3"}
library(ellmer)

chat <- chat_openai()
```
:::

::: col
[![](../assets/icons/python-icon-color.svg){height="36px" alt="Python"} chatlas]{.flex .items-center .gap-1}

```{.python code-line-numbers="3"}
import chatlas

chat = chatlas.ChatOpenAI()
```
:::
:::

## Hello, ellmer and chatlas! {transition="fade"}

::: {.smaller style="--code-font-size: 0.66em"}
::: col
[![](../assets/icons/r-color.svg){height="36px" alt="R"} ellmer]{.flex .items-center .gap-1}

```{.r code-line-numbers="5"}
library(ellmer)

chat <- chat_openai()

chat$chat("Tell me a joke about R.")
```
:::

::: col
[![](../assets/icons/python-icon-color.svg){height="36px" alt="Python"} chatlas]{.flex .items-center .gap-1}

```{.python code-line-numbers="5"}
import chatlas

chat = chatlas.ChatOpenAI()

chat.chat("Tell me a joke about Python.")
```
:::
:::

## Hello, ellmer and chatlas! {transition="fade"}

::: {.smaller style="--code-font-size: 0.66em"}
::: col
[![](../assets/icons/r-color.svg){height="36px" alt="R"} ellmer]{.flex .items-center .gap-1}

```{.r code-line-numbers="5-7"}
library(ellmer)

chat <- chat_openai()

chat$chat("Tell me a joke about R.")
#> Why did the R programmer go broke?
#> Because he kept using `sample()` and lost all his data!
```
:::

::: col
[![](../assets/icons/python-icon-color.svg){height="36px" alt="Python"} chatlas]{.flex .items-center .gap-1}

```{.python code-line-numbers="5-7"}
import chatlas

chat = chatlas.ChatOpenAI()

chat.chat("Tell me a joke about Python.")
#> Why do Python programmers prefer using snakes as pets?
#> Because they don't mind the indentation!
```
:::
:::

::: {.fragment .absolute top=100 right=50 class="w-50 ba b--dark-red bg-washed-red dark-red pa3 br2 flex items-start tr"}
❓ What are the **user** and **assistant** roles in this example?
:::

## Hello, ellmer and chatlas! {transition="fade"}

::: {.smaller style="--code-font-size: 0.66em"}
::: col
[![](../assets/icons/r-color.svg){height="36px" alt="R"} ellmer]{.flex .items-center .gap-1}

```r
chat
```
```{.markdown code-line-numbers=false}
<Chat OpenAI/gpt-4.1 turns=2 tokens=14/29 $0.00>
── user [14] ────────────────────────────────────────
Tell me a joke about R.
── assistant [29] ───────────────────────────────────
Why did the R programmer go broke?

Because he kept using `sample()` and lost all his data!
```
:::

::: col
[![](../assets/icons/python-icon-color.svg){height="36px" alt="Python"} chatlas]{.flex .items-center .gap-1}

```python
print(chat)
```
:::
:::

## Hello, ellmer and chatlas! {transition="fade"}

::: {.smaller style="--code-font-size: 0.66em"}
::: col
[![](../assets/icons/r-color.svg){height="36px" alt="R"} ellmer]{.flex .items-center .gap-1}

```r
chat
```
:::

::: col
[![](../assets/icons/python-icon-color.svg){height="36px" alt="Python"} chatlas]{.flex .items-center .gap-1}

```python
print(chat)
```
```{.markdown code-line-numbers=false}
## 👤 User turn:

Tell me a joke about Python.

## 🤖 Assistant turn:

Why do Python programmers prefer using snakes as pets?

Because they don't mind the indentation!
```
:::
:::

::: {.fragment .absolute top=100 right=50 class="w-50 ba b--dark-red bg-washed-red dark-red pa3 br2 flex items-start tr"}
❓ What about the **system prompt?**
:::

## Hello, ellmer and chatlas! {transition="fade"}

::: {.smaller style="--code-font-size: 0.66em"}
::: col
[![](../assets/icons/r-color.svg){height="36px" alt="R"} ellmer]{.flex .items-center .gap-1}

```{.r code-line-numbers="3-5"}
library(ellmer)

chat <- chat_openai(
  system_prompt = "You are a dad joke machine."
)

chat$chat("Tell me a joke about R.")
```
:::

::: col
[![](../assets/icons/python-icon-color.svg){height="36px" alt="Python"} chatlas]{.flex .items-center .gap-1}

```{.python code-line-numbers="3-5"}
import chatlas

chat = chatlas.ChatOpenAI(
  system_prompt="You are a dad joke machine."
)

chat.chat("Tell me a joke about Python.")
```
:::
:::

## Hello, ellmer and chatlas! {transition="fade"}

::: {.smaller style="--code-font-size: 0.66em"}
[![](../assets/icons/r-color.svg){height="36px" alt="R"} ellmer]{.flex .items-center .gap-1}

```{.r code-line-numbers="7"}
library(ellmer)

chat <- chat_openai(
  system_prompt = "You are a dad joke machine."
)

chat$chat("Tell me a joke about R.")
```

::: fragment
```{.markdown code-line-numbers=false}
Why did the letter R get invited to all the pirate parties?

Because it always knows how to *arr-r-ive* in style!
```
:::
:::

## Hello, ellmer and chatlas! {transition="fade"}

::: {.smaller style="--code-font-size: 0.66em"}
[![](../assets/icons/r-color.svg){height="36px" alt="R"} ellmer]{.flex .items-center .gap-1}

```{.r code-line-numbers="7"}
chat
```

::: fragment
```{.markdown code-line-numbers=false}
<Chat OpenAI/gpt-4.1 turns=3 tokens=25/28 $0.00>
── system [0] ───────────────────────────────────────
You are a dad joke machine.
── user [25] ────────────────────────────────────────
Tell me a joke about R.
── assistant [28] ───────────────────────────────────
Why did the letter R get invited to all the pirate parties?

Because it always knows how to *arr-r-ive* in style!
```
:::
:::


# Your Turn `02_word-game` {.slide-your-turn}

1. Set up a `chat` with a system prompt:

   > You are playing a word guessing game. At each turn, guess the word and tell us what it is.

2. **Ask:** _In British English, guess the word for the person who lives next door._

3. **Ask:** _What helps a car move smoothly down the road?_

4. Create a new, empty `chat` and ask the second question again.

5. How do the answers to 3 and 4 differ? Why?

# Demo: `clearbot` {.slide-demo style="--code-font-size: 0.66em"}

👨‍💻 [_demos/03_clearbot/app.py]{.code .b .purple}

**System prompt:**

```{.markdown code-line-numbers=false}
You are playing a word guessing game. At each turn, guess the word and tell us what it is.
```

**First question:**

```{.markdown code-line-numbers=false}
In British English, guess the word for the person who lives next door.
```

**Second question:**

```{.markdown code-line-numbers=false}
What helps a car move smoothly down the road?
```

# [How to talk to robots]{.dib .bg-black .ph4 .white style="position: relative; top: -3em;"} {.no-invert-dark-mode background-image="assets/andy-kelly-0E_vhMVqL9g-unsplash.jpg" background-size="cover" background-position="center"}

## {.center}

![](assets/intro-conversation-01.png)
![](assets/intro-conversation-02.png){.fragment}

::: notes
Those conversations often look something like this:

You ask a question and ChatGPT responds.
You can keep talking to ChatGPT and it keeps responding,
almost like you're having a conversation with a person.
:::

## {.center background-image="assets/not-a-conversation2.png" background-size="contain" background-color="#E3CCA4"}

::: notes
But this is not a conversation.

It certainly _feels_ like a conversation,
but something else is happening behind the scenes.
:::


## When you talk to ChatGPT {.center}

::: incremental
1. You write some words

2. The ChatGPT continues writing words

3. You think you're having a conversation
:::


## ChatGPT {.center}

::: r-fit-text
**Chat**ting with a **G**enerative **P**re-trained **T**ransformer
:::

::: {.fragment .r-fit-text}
**LLM** &rarr; **L**arge **L**anguage **M**odel
:::

::: notes
GPT is a type of LLM, but not all LLMs are GPTs.

LLM: Deep learning on huge amounts of text data to understand and generate human language.
GPT: Specific architectural design of LLMs.

From now on, we'll use **LLM** to refer to any large language model.
:::

## How to make an LLM {.center}

::: {.columns}
::: {.column}
**If you read everything<br>ever written...**

* Books and stories

* Websites and articles

* Poems and jokes

* Questions and answers
:::
::: {.column .fragment}
<br>**...then you could...**

- Answer questions
- Write stories
- Tell jokes
- Explain things
- Translate into any language
:::
:::


## The cat sat in the ____ {.center}

::: incremental
::: {.columns style="font-size: 6rem;"}
::: {.column}
* 🎩
* 🛌
* 📦
:::
::: {.column}
* 🪟
* 🛒
* 👠
:::
:::
:::

::: notes
The breakthrough insight is the attention mechanism in the Transformer architecture.
Instead of processing text sequentially (word by word),
GPT models can look at all words in a sequence simultaneously
and understand how they relate to each other.

The attention mechanism allows the model to:

* Weigh the importance of every word in relation to every other word
* Capture long-range dependencies that traditional models missed
* Process sequences in parallel rather than sequentially
:::

## Actually: tokens, not words {.center}

::: {.incremental}
- Fundamental units of information for LLMs
- Words, parts of words, or individual characters
  - "hello" → 1 token
  - "unconventional" → 3 tokens: `un|con|ventional`
- Important for:
  - Model input/output limits
  - [API pricing](https://llmpricecheck.com/calculator/) is usually by token
- Not just words, but images can be tokenized too
:::

## {background-image="assets/token-approximations.png" background-size="contain" visibility="hidden"}

::: footer
<https://llm-stats.com/>
:::

::: notes
The newest and biggest models from OpenAI and Google
can handle 1 million input tokens at once.

Which is kind of like saying that you could paste

* 30 hours of podcasts
* 1,000 pages of a book
* 60,000 lines of code

into the chat window and the model could "pay attention" to it all.

1M is the upper limit currently, most models accept up to around 200k tokens.
:::


# Demo: <br> `token-possibilities` {.slide-demo style="--code-font-size: 0.66em"}

👨‍💻 [_demos/04_token-possibilities/app.R]{.code .b .purple}

# [Programming is fun, but I kind of like ChatGPT...]{.bg-white .f1 .normal .relative style="top: -14px"} {background-image="assets/shinychat-input-empty.png" background-size="contain" background-position="center"}

# [ellmer and chatlas can do that, too!]{.bg-white .f1 .normal .relative style="top: -14px"} {background-image="assets/shinychat-input-focused.png" background-size="contain" background-position="center"}

## {.center}

::: {.table-no-border}
| | Console | Browser |
|:---:|:---:|:---:|
| ![](../assets/logos/ellmer.png){height="4em" alt="ellmer"} | `live_console(chat)` | `live_browser(chat)` |
| ![](../assets/logos/chatlas.png){height="4em" alt="chatlas"} | `chat.console()` | `chat.app()` |

: {tbl-colwidths="[20,40,40]"}
:::

::: notes
As we'll see, everything we're going to do today can be done
in either R or Python and ellmer and chatlas are designed to be similar.

But they're in different languages and those languages have different
conventions and idioms, so they're not completely identical. If they were,
one or the other would start to feel unnatural to use.
:::

# Your Turn `05_live` {.slide-your-turn}

::: {style="--li-margin: 0.66em;"}
1. Your job: write a groan-worthy roast of Hadley Wickham

2. Bonus points for puns, rhymes, and one-liners

3. Don't be mean

4. Share your best on Discord 😉
:::

{{< countdown 3:00 top="-1em" >}}

# [shinychat]{.hidden} {.no-invert-dark-mode background-image="assets/shinychat-robot2.png" background-size="cover" background-position="center"}

## {.center}

::: {.easy-columns .tc}
![](../assets/logos/shinychat.png){style="max-width: 100%; max-height: 500px" alt="shinychat"}

![](../assets/logos/ellmer.png){style="max-width: 100%; max-height: 500px" alt="ellmer"}
:::

## shinychat in R {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

```{=html}
<style>
:root {
  --shinychat-code-font-size: 0.66em;
}
</style>
```

Start with the `shinyapp` snippet

```{.r}
library(shiny)
library(bslib)

ui <- page_fillable(

)

server <- function(input, output, session) {

}

shinyApp(ui, server)
```

::: footer
<https://www.garrickadenbuie.com/blog/shiny-new-bslib-snippet/>
:::

## shinychat in R {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Load `{shinychat}` and `{ellmer}`

```{.r code-line-numbers="3-4"}
library(shiny)
library(bslib)
library(shinychat)
library(ellmer)

ui <- page_fillable(

)

server <- function(input, output, session) {

}

shinyApp(ui, server)
```

## shinychat in R {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Use the shinychat chat module

```{.r code-line-numbers="7,11"}
library(shiny)
library(bslib)
library(shinychat)
library(ellmer)

ui <- page_fillable(
  chat_mod_ui("chat")
)

server <- function(input, output, session) {
  chat_mod_server("chat")
}

shinyApp(ui, server)
```

## shinychat in R {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Create and hook up a chat client to use in the app

```{.r code-line-numbers="12-13"}
library(shiny)
library(bslib)
library(shinychat)
library(ellmer)


ui <- page_fillable(
  chat_mod_ui("chat")
)

server <- function(input, output, session) {
  client <- chat_openai()
  chat_mod_server("chat", client)
}

shinyApp(ui, server)
```


## shinychat in R {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Why is this not a great idea?

```{.r code-line-numbers="6-14"}
library(shiny)
library(bslib)
library(shinychat)
library(ellmer)

client <- chat_openai()

ui <- page_fillable(
  chat_mod_ui("chat")
)

server <- function(input, output, session) {
  chat_mod_server("chat", client)
}

shinyApp(ui, server)
```

::: notes
Copy and paste the app code into an R session and run it!
:::

## {.center}

::: {.easy-columns .tc}
![](../assets/logos/shinychat.png){style="max-width: 100%; max-height: 500px" alt="shinychat"}

![](../assets/logos/chatlas.png){style="max-width: 100%; max-height: 500px" alt="chatlas"}
:::

## shinychat in Python {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Start with the `shinyapp` snippet

```{.python}
from shiny import App, reactive, render, req, ui

app_ui = ui.page_fillable(
  ui.input_slider("n", "N", 0, 100, 20),
  ui.output_text_verbatim("txt"),
)


def server(input, output, session):
  @render.text
  def txt():
    return f"n*2 is {input.n() * 2}"


app = App(app_ui, server)
```

## shinychat in Python {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Remove the parts we don't need

```{.python}
from shiny import App, ui

app_ui = ui.page_fillable(

)

def server(input, output, session):
    pass

app = App(app_ui, server)
```

## shinychat in Python {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Create a chatlas chat client

```{.python code-line-numbers="1,9"}
import chatlas
from shiny import App, ui

app_ui = ui.page_fillable(

)

def server(input, output, session):
    client = chatlas.ChatOpenAI()

app = App(app_ui, server)
```

## shinychat in Python {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

Add the Chat UI and server logic (client and chat aren't connected yet!)

```{.python code-line-numbers="5,10"}
import chatlas
from shiny import App, ui

app_ui = ui.page_fillable(
    ui.chat_ui("chat")
)

def server(input, output, session):
    client = chatlas.ChatOpenAI()
    chat = ui.Chat("chat")

app = App(app_ui, server)
```

## shinychat in Python {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

When the user submits a message...

```{.python code-line-numbers="12-15"}
import chatlas
from shiny import App, ui

app_ui = ui.page_fillable(
    ui.chat_ui("chat")
)

def server(input, output, session):
    client = chatlas.ChatOpenAI()
    chat = ui.Chat("chat", client)

    @chat.on_user_submit
    async def _(user_input: str):
        # Send input to LLM
        # Send response back to UI

app = App(app_ui, server)
```

## shinychat in Python {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

we'll send the input to the LLM...

```{.python code-line-numbers="14"}
import chatlas
from shiny import App, ui

app_ui = ui.page_fillable(
    ui.chat_ui("chat")
)

def server(input, output, session):
    client = chatlas.ChatOpenAI()
    chat = ui.Chat("chat", client)

    @chat.on_user_submit
    async def _(user_input: str):
        response = await client.stream_async(user_input)
        # Send response back to UI

app = App(app_ui, server)
```

## shinychat in Python {style="--code-font-size: var(--shinychat-code-font-size)" auto-animate=true}

...and then stream the response back to the UI.

```{.python code-line-numbers="15"}
import chatlas
from shiny import App, ui

app_ui = ui.page_fillable(
    ui.chat_ui("chat")
)

def server(input, output, session):
    client = chatlas.ChatOpenAI()
    chat = ui.Chat("chat", client)

    @chat.on_user_submit
    async def _(user_input: str):
        response = await client.stream_async(user_input)
        await chat.append_message_stream(response)

app = App(app_ui, server)
```

# Your Turn `06_word-games` {.slide-your-turn}

1. I've set up the basic Shiny app snippet and a system prompt.

2. Your job: create a chatbot that plays the word guessing game with you.

3. The twist: this time, you're guessing the word.

{{< countdown 7:00 top="-1em" >}}


## Interpolation in R

```{r}
#| echo: true
#| output-location: fragment
#| code-line-numbers: "3|5-7"

library(ellmer)

words <- c("elephant", "bicycle", "sandwich")

interpolate(
  "The secret word is {{ sample(words, 1) }}."
)
```

## Interpolation in R

```{r}
#| echo: true
#| code-line-numbers: "5-7"

library(ellmer)

words <- c("elephant", "bicycle", "sandwich")

interpolate(
  "The secret word is {{ words }}."
)
```

## Interpolation in Python

```{python}
#| echo: true
#| output-location: fragment
#| code-line-numbers: "3|5"

import random

words = ["elephant", "bicycle", "sandwich"]

f"The secret word is {random.choice(words)}."
```
